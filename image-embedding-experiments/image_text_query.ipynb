{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Define Reference Images\n",
    "reference_images = [\n",
    "    {\n",
    "        \"url\": \"https://images.unsplash.com/reserve/bOvf94dPRxWu0u3QsPjF_tree.jpg?q=80&w=1776&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\",\n",
    "        \"tags\": [\"tree\", \"nature\"],\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://images.unsplash.com/photo-1437964706703-40b90bdf563b?q=80&w=1974&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\",\n",
    "        \"tags\": [\"tree\", \"forest\"],\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://images.unsplash.com/photo-1444312645910-ffa973656eba?q=80&w=1887&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\",\n",
    "        \"tags\": [\"rock\", \"outdoor\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Load CLIP Model\n",
    "model_ckpt = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_ckpt)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_ckpt)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "# Compute Reference Image Embeddings\n",
    "ref_embeddings = []\n",
    "for ref in reference_images:\n",
    "    url = ref[\"url\"]\n",
    "    tags = ref[\"tags\"]\n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        if resp.status_code == 200:\n",
    "            img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "            \n",
    "            # Preprocess for CLIP\n",
    "            inputs = clip_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get image embeddings and normalize\n",
    "                image_embeds = clip_model.get_image_features(**inputs)\n",
    "                image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "            \n",
    "            ref_embeddings.append({\n",
    "                \"image_url\": url,\n",
    "                \"tags\": tags,\n",
    "                \"embedding\": image_embeds.squeeze(0)  # shape: (512,)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Failed to fetch image {url} - status: {resp.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {url}: {e}\")\n",
    "\n",
    "print(\"Reference image embeddings computed:\", len(ref_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Function to perform text-based querying\n",
    "def query_images_with_text(query_text, top_k=3):\n",
    "    \"\"\"\n",
    "    Given a text query, find and display the top_k most similar images from the reference set.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The text query string.\n",
    "        top_k (int): Number of top similar images to retrieve.\n",
    "    \"\"\"\n",
    "    # 1. Embed the text query using CLIP's text encoder\n",
    "    text_inputs = clip_processor(text=[query_text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_embeds = clip_model.get_text_features(**text_inputs)\n",
    "        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "    \n",
    "    # 2. Compute similarity scores with all reference image embeddings\n",
    "    similarities = []\n",
    "    for ref in ref_embeddings:\n",
    "        sim_score = torch.matmul(text_embeds, ref[\"embedding\"])\n",
    "        similarities.append(sim_score.item())\n",
    "    \n",
    "    # 3. Get top_k indices based on similarity scores\n",
    "    top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_k]\n",
    "    \n",
    "    # 4. Display the query and top_k similar images\n",
    "    num_images = top_k + 1  # +1 for the query description\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(5 * num_images, 5))\n",
    "    \n",
    "    # Display the query text\n",
    "    axes[0].text(0.5, 0.5, f\"Query:\\n'{query_text}'\", fontsize=12, ha='center', wrap=True)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(\"Text Query\")\n",
    "    \n",
    "    # Display the top_k similar images\n",
    "    for idx, ref_idx in enumerate(top_k_indices):\n",
    "        ref = ref_embeddings[ref_idx]\n",
    "        sim_score = similarities[ref_idx]\n",
    "        img_url = ref[\"image_url\"]\n",
    "        tags = ref[\"tags\"]\n",
    "        \n",
    "        try:\n",
    "            resp = requests.get(img_url)\n",
    "            if resp.status_code == 200:\n",
    "                img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "                axes[idx + 1].imshow(img)\n",
    "                axes[idx + 1].axis(\"off\")\n",
    "                axes[idx + 1].set_title(f\"Similarity: {sim_score:.3f}\\nTags: {tags}\")\n",
    "            else:\n",
    "                axes[idx + 1].set_title(f\"Failed to fetch image.\\nStatus: {resp.status_code}\")\n",
    "                axes[idx + 1].axis(\"off\")\n",
    "        except Exception as e:\n",
    "            axes[idx + 1].set_title(f\"Error loading image.\\n{e}\")\n",
    "            axes[idx + 1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget for text input\n",
    "text_query = widgets.Text(\n",
    "    value='rock',\n",
    "    placeholder='Enter query text',\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Widget for selecting top_k\n",
    "top_k_slider = widgets.IntSlider(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=len(ref_embeddings),\n",
    "    step=1,\n",
    "    description='Top K:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Function to update the display based on widget interaction\n",
    "def on_query_change(change):\n",
    "    query = change['new']\n",
    "    query_images_with_text(query, top_k_slider.value)\n",
    "\n",
    "# Function to update when top_k changes\n",
    "def on_topk_change(change):\n",
    "    query = text_query.value\n",
    "    query_images_with_text(query, change['new'])\n",
    "\n",
    "# Attach listeners\n",
    "text_query.observe(on_query_change, names='value')\n",
    "top_k_slider.observe(on_topk_change, names='value')\n",
    "\n",
    "# Display widgets\n",
    "display(text_query, top_k_slider)\n",
    "\n",
    "# Initial display\n",
    "query_images_with_text(text_query.value, top_k_slider.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
